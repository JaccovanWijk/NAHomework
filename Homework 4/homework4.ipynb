{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7deae09c-da1d-4d16-8551-b00dd566e8f1",
   "metadata": {},
   "source": [
    "# About python imports\n",
    "\n",
    "No imports are specified as in previous homeworks, please insert the necessary python code yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608c1096-a095-4731-b9e9-df81c9f59c22",
   "metadata": {},
   "source": [
    "-----\n",
    "# Exercise 1 (3 points)\n",
    "Write a program implementing Rayleigh quotient iteration for computing an eigenvalue and corresponding eigenvector of a matrix. Test your program on the matrix \n",
    "$$\n",
    "  A = \\begin{bmatrix} 6 & 2 & 1 \\\\ 2& 3 & 1 \\\\ 1 & 1 & 1  \\end{bmatrix}\n",
    "$$\n",
    "using a random starting vector. Let the program create output that shows the convergence behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c19f0e-4aea-4b93-b4c4-44b3b78b4443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4fb906-e2a8-44c6-94ed-f4c1c69f8105",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Automatic differentiation using JAX\n",
    "\n",
    "In applications of rootfinding, computing the derivative is often a problematic step. For example, the function for which zeros are sought might be given by a complicated computer program. \n",
    "\n",
    "Automatic differentiation is a set of techniques to evaluate the derivative of a function specified by a computer program. See the [wikipedia page](https://en.wikipedia.org/wiki/Automatic_differentiation) on this topic.\n",
    "\n",
    "[JAX](https://github.com/google/jax) is a software package that implements automatic differentiation as well as other functionality. [The documentation is here](https://jax.readthedocs.io/en/latest/). The idea of this exercise is to use JAX for obtaining derivative functions.\n",
    "\n",
    "To use JAX, there are two options:\n",
    "- install JAX. The installation of JAX is described on the Github page, see https://github.com/google/jax#installation.  **It appears that installation under Windows is not supported. According to the internet, one may use the Windows Subsystem for Linux, but I haven't tested this.**\n",
    "- run your python notebook on the google colab environment. The google colab environment is at https://colab.research.google.com. In the google colab environment, the JAX package is available. \n",
    "\n",
    "There is some material online about JAX, see for example\n",
    "https://medium.com/swlh/solving-optimization-problems-with-jax-98376508bd4f\n",
    "(LATEX-pdf version here\n",
    "https://github.com/mazy1998/Solving-Optimization-Problems-with-JAX/blob/master/Opitimization_with_jax.pdf)\n",
    "or \n",
    "https://www.kaggle.com/aakashnain/tf-jax-tutorials-part1.\n",
    "\n",
    "The result is that for many functions, the derivative can be automatically computed. We will show this for a vector valued function $\\mathbb{R}^2 \\to \\mathbb{R}^2$. \n",
    "\n",
    "The first step is to import some functions from the package JAX. Notice that JAX has its own version of numpy. Here we import it as `jnp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2941c681-32c8-4d6a-b941-cd1ca6a8ed55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import jacfwd, jacrev\n",
    "\n",
    "# depending on the application, more imports are needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd7b92c-1234-4e33-8327-73427a0edcab",
   "metadata": {},
   "source": [
    "JAX implements forward and reverse mode automatic differentiation. The commands are `jacfwd` and `jacrev` (jac stands for Jacobian). The [wikipedia page on automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) briefly introduces forward and reverse mode automatic differentiation. Here we just mention that forward accumulation is more efficient than reverse accumulation for functions $f : \\mathbb{R}^n \\to \\mathbb{R}^m$ with $m \\gg n$ as only $n$ sweeps are necessary, compared to $m$ sweeps for reverse accumulation and that reverse accumulation is more efficient than forward accumulation for functions $f : \\mathbb{R}^n \\to \\mathbb{R}^m$ with $m \\ll n$ as only $n$ sweeps are necessary.\n",
    "\n",
    "In the following cell a simple function is defined and differentiated. Note that JAX has its own array type, `jax.numpy.array` (because of the line `import jax.numpy as jnp` we write this as `jnp.array`), that output is in 32 bits floating point format in a different array type `DeviceArray` (JAX has a preference for the 32 bits floating point format and you are allowed to use it in this exercise). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afa2d99c-adcd-45c6-a786-9653b0d2a390",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([2., 4.], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def circle(x): return x[0]**2 + x[1]**2\n",
    "J = jacfwd(circle)\n",
    "J(jnp.array([1.0 ,2.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fbe136-c903-4a13-867d-b6738275a2bc",
   "metadata": {},
   "source": [
    "It is allowed to take derivatives of derivatives, so that a second derivative matrix can be obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f909ba17-e90f-44d1-a79b-0c1e2e03a84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 0.]\n",
      " [0. 2.]]\n"
     ]
    }
   ],
   "source": [
    "def hessian(f): return jacfwd(jacrev(f))\n",
    "H = hessian(circle)\n",
    "myMatrix = H(jnp. array ([1.0 ,2.0]) )\n",
    "print(myMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c586ff-da85-4772-8596-e5558a95a6b3",
   "metadata": {},
   "source": [
    "Although this is not the standard `numpy.ndarray` format, it appears however that this format can be used in linear algebra operations such as solve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43cbca1e-3c02-47ce-aeee-bcac0dd35443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25, 1.  ], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myVector = jnp.array([0.5, 2.0])\n",
    "\n",
    "import scipy.linalg as la\n",
    "la.solve(myMatrix, myVector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0488e28-7322-46ca-a2a3-6f2b9e8e81d0",
   "metadata": {},
   "source": [
    "It is easy to define vector valued functions, by returning an `jax.numpy.array` object. \n",
    "\n",
    "When using functions such as $\\sin$ and $\\cos$ and $\\exp$ one must be careful. One must use the functions `jax.numpy.sin`, `jax.numpy.cos`, etc. (and not `math.sin` etc.). We define a test function $f(x_1,x_2) = [x_1 \\exp(x_2), x_1+x_2]$ using $\\exp$ and show that it can be differentiated. Note that the derivative matrix is $\\begin{bmatrix} \\exp(x_2) & x_1 \\exp(x_2) \\\\ 1 & 1 \\end{bmatrix}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e99c6fd-109d-42ed-8e2a-096d103ea00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values:\n",
      "[5.4365635 3.       ]\n",
      "jacobian matrix:\n",
      "[[2.7182817 5.4365635]\n",
      " [1.        1.       ]]\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return jnp.array([x[0] * jnp.exp(x[1]), x[0] + x[1]])\n",
    "\n",
    "print(\"values:\")\n",
    "print(f(jnp.array([2.0,1.0])))\n",
    "\n",
    "Df = jacfwd(f)\n",
    "print(\"jacobian matrix:\")\n",
    "print(Df(jnp.array([2.0,1.0])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbb295f-5416-4cc4-b59b-0da5a3abfc4c",
   "metadata": {},
   "source": [
    "# Exercise 2 (rootfinding with automatic differentiation, 3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71362439-2461-4dd2-ba65-ee58dd4acf98",
   "metadata": {},
   "source": [
    "## (a)\n",
    "Create a Python function to apply Newton's method in multiple dimensions. Create a stopping criterion, such that your method automatically stops when one of the following conditions is satisfied: (i) the size of the function is below a specified tolerance; (ii) the difference in two subsequent iterates $\\mathbf{x}_k$ is below a specified tolerance; (iii) the number of iterations reaches a specified limit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd2d71a-e830-4636-a052-c3b455a0ab99",
   "metadata": {},
   "source": [
    "## (b)\n",
    "Solve Computer Exercise 5.19 using Newton's method and automatic differentiation. (N.B. Do not choose the starting point equal to a solution.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a3e752a-4e45-40b2-b1f2-75a17d311a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb585a79-8d3c-4d8e-9889-ddca952dbb85",
   "metadata": {},
   "source": [
    "Explanation using $\\LaTeX$ here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b83806-20bd-4c7f-9c8d-5bb5de23c231",
   "metadata": {},
   "source": [
    "# Exercise 3 (3 points)\n",
    "\n",
    "## (a)\n",
    "\n",
    "Consider the system \n",
    "$$\\begin{aligned}(x_1+3)(x_2^3-7) + 18 = {}& 0 \\\\\n",
    "\\sin(x_2 e^{x_1} -1) = {}& 0 .\n",
    "\\end{aligned}$$\n",
    "Solve this system using Newton's method with starting \n",
    "point $\\mathbf{x}_0 = [ 0.5 \\;\\; 1.4 ] ^T$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69553d80-f22d-46ab-ba2e-60efa972e609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb2fd2b-66b7-4bdc-8793-4f3064d2339f",
   "metadata": {},
   "source": [
    "## (b)\n",
    "Write a program based on Broyden's method to solve the same system with the same starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05feae8e-0248-47a8-bb51-7b886f4f6e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7063fe1-dab1-41d1-85e5-875a98c2f24c",
   "metadata": {},
   "source": [
    "## (c)\n",
    "Compare the convergence rates of the two methods by computing the error at each iteration and appropriately analysing these errors, given that the exact solution is $\\mathbf{x}^* = [ 0 \\;\\; 1 ]^T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e04b39-1da7-4882-91a0-a649b271bfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67ed772-59bf-43fa-92c5-b99384421b61",
   "metadata": {},
   "source": [
    "Explanation using $\\LaTeX$ here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
